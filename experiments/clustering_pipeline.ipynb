{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbb5ff28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\akuma\\Desktop\\Projects\\Whatsapp_Moments_Aggregator\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import math\n",
    "from typing import Tuple, List, Dict, Any\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# NLP & embedding libs\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import umap\n",
    "import hdbscan\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "# Image libs\n",
    "from PIL import Image\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e32708d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d8fb866",
   "metadata": {},
   "outputs": [],
   "source": [
    "MONGO_URI = os.getenv(\"MONGO_URI\", \"mongodb://localhost:27017\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\", \"life_db\")\n",
    "CHATS_COLLECTION = os.getenv(\"CHATS_COLLECTION\", \"chats\")\n",
    "MEDIA_COLLECTION = os.getenv(\"MEDIA_COLLECTION\", \"media\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "724b9cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = os.getenv(\"OUTPUT_DIR\", \"pipeline_outputs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cee6f29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_EMBED_MODEL = os.getenv(\n",
    "    \"TEXT_EMBED_MODEL\", \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    ")\n",
    "CLIP_MODEL = os.getenv(\"CLIP_MODEL\", \"clip-ViT-B-32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8e8abbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_aggregations(chats: pd.DataFrame, media: pd.DataFrame, tz: str = None):\n",
    "    out = {}\n",
    "    for freq, name in [(\"D\",\"day\"), (\"W\",\"week\"), (\"M\",\"month\"), (\"Y\",\"year\")]:\n",
    "        cagg = chats.groupby(pd.Grouper(key=\"Datetime\", freq=freq)).agg(\n",
    "            msg_count=(\"Message\", \"count\"),\n",
    "            total_chars=(\"CharCount\", \"sum\"),\n",
    "            emoji_count=(\"EmojiCount\", \"sum\"),\n",
    "            avg_words=(\"WordCount\", \"mean\")\n",
    "        ).fillna(0)\n",
    "        magg = media.groupby(pd.Grouper(key=\"Datetime\", freq=freq)).agg(\n",
    "            media_count=(\"FileName\", \"count\"),\n",
    "            has_gps=(\"HasGPS\", \"sum\")\n",
    "        ).fillna(0)\n",
    "        df = cagg.join(magg, how=\"outer\").fillna(0)\n",
    "        df = df.reset_index().rename(columns={\"Datetime\": \"period_start\"})\n",
    "        for col in [\"msg_count\", \"media_count\"]:\n",
    "            if df[col].std(ddof=0) > 0:\n",
    "                df[f\"{col}_z\"] = (df[col] - df[col].mean()) / (df[col].std(ddof=0) + 1e-9)\n",
    "            else:\n",
    "                df[f\"{col}_z\"] = 0.0\n",
    "        df[\"activity_score\"] = 0.7 * df[\"msg_count_z\"] + 0.3 * df[\"media_count_z\"]\n",
    "        lo = df[\"activity_score\"].quantile(0.2)\n",
    "        hi = df[\"activity_score\"].quantile(0.8)\n",
    "        df[\"period_label\"] = df[\"activity_score\"].apply(lambda v: \"Quiet\" if v <= lo else (\"Active\" if v >= hi else \"Typical\"))\n",
    "        out[name] = df\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0161ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# 2) Topic-based Clustering (Chats)\n",
    "# -------------------------\n",
    "def topic_cluster_chats(chats: pd.DataFrame,\n",
    "                        text_col: str = \"CleanMessage\",\n",
    "                        min_len: int = 3,\n",
    "                        embed_model_name: str = TEXT_EMBED_MODEL,\n",
    "                        umap_n_neighbors: int = 15,\n",
    "                        umap_min_dist: float = 0.0,\n",
    "                        umap_dims: int = 64,\n",
    "                        hdb_min_cluster_size: int = 30,\n",
    "                        use_pca: bool = True,\n",
    "                        pca_dim: int = 64):\n",
    "    \"\"\"\n",
    "    Input: chats DataFrame with a CleanMessage column\n",
    "    Output: messages_df with added 'topic_id' and topic_summary DataFrame\n",
    "    \"\"\"\n",
    "    df = chats.copy()\n",
    "    if text_col not in df.columns:\n",
    "        raise ValueError(f\"'{text_col}' not in chats DataFrame\")\n",
    "    df[text_col] = df[text_col].fillna(\"\").astype(str)\n",
    "    df[\"len\"] = df[text_col].str.len()\n",
    "    df = df[df[\"len\"] >= min_len].reset_index(drop=True)\n",
    "    if df.empty:\n",
    "        return df, pd.DataFrame()\n",
    "\n",
    "    print(\"[topic] loading text embedding model:\", embed_model_name)\n",
    "    txt_model = SentenceTransformer(embed_model_name)\n",
    "    texts = df[text_col].tolist()\n",
    "    embeddings = txt_model.encode(texts, batch_size=64, show_progress_bar=True, normalize_embeddings=True)\n",
    "\n",
    "    # optional PCA\n",
    "    if use_pca and embeddings.shape[1] > pca_dim:\n",
    "        print(f\"[topic] reducing dims with PCA -> {pca_dim}\")\n",
    "        pca = PCA(n_components=pca_dim, random_state=42)\n",
    "        embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "    # UMAP to get compact low-d representation for HDBSCAN\n",
    "    print(\"[topic] running UMAP\")\n",
    "    reducer = umap.UMAP(n_neighbors=umap_n_neighbors, min_dist=umap_min_dist, n_components=umap_dims, metric=\"cosine\", random_state=42)\n",
    "    X_umap = reducer.fit_transform(embeddings)\n",
    "\n",
    "    # HDBSCAN clustering\n",
    "    print(\"[topic] running HDBSCAN\")\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=hdb_min_cluster_size, metric=\"euclidean\", cluster_selection_method=\"eom\")\n",
    "    labels = clusterer.fit_predict(X_umap)\n",
    "    df[\"topic_id\"] = labels\n",
    "\n",
    "    # Topic summaries: top keywords per cluster (tf-idf)\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    topic_list = []\n",
    "    for t in sorted(set(labels)):\n",
    "        mask = df[\"topic_id\"] == t\n",
    "        texts_in_cluster = df.loc[mask, text_col].tolist()\n",
    "        if not texts_in_cluster:\n",
    "            topic_label = \"Misc\"\n",
    "            n_msgs = 0\n",
    "            first_seen = None\n",
    "            last_seen = None\n",
    "        else:\n",
    "            n_msgs = len(texts_in_cluster)\n",
    "            first_seen = df.loc[mask, \"Datetime\"].min() if \"Datetime\" in df.columns else None\n",
    "            last_seen = df.loc[mask, \"Datetime\"].max() if \"Datetime\" in df.columns else None\n",
    "            # TF-IDF top words\n",
    "            try:\n",
    "                tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=3000, stop_words=\"english\")\n",
    "                X = tfidf.fit_transform(texts_in_cluster)\n",
    "                scores = np.asarray(X.mean(axis=0)).ravel()\n",
    "                top_idx = scores.argsort()[::-1][:6]\n",
    "                terms = np.array(tfidf.get_feature_names_out())[top_idx]\n",
    "                topic_label = \", \".join(terms[:4])\n",
    "            except Exception:\n",
    "                # fallback simple freq words\n",
    "                words = \" \".join(texts_in_cluster).lower().split()\n",
    "                common = pd.Series(words).value_counts().head(5).index.tolist()\n",
    "                topic_label = \" \".join(common)\n",
    "\n",
    "        topic_list.append({\n",
    "            \"topic_id\": int(t),\n",
    "            \"n_msgs\": int(n_msgs),\n",
    "            \"topic_label\": topic_label,\n",
    "            \"first_seen\": str(first_seen),\n",
    "            \"last_seen\": str(last_seen)\n",
    "        })\n",
    "\n",
    "    topic_summary = pd.DataFrame(topic_list).sort_values(\"n_msgs\", ascending=False).reset_index(drop=True)\n",
    "    return df, topic_summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
